{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we introduce and apply a range of supervised and unsupervised text analysis techniques to social science data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use Python to analyse text data relating to charitable activities.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data preprocessing problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 40-60 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand and apply common supervised and unsupervised text analysis techniques to social science data.\n",
    "    3. Be able to use Python for performing text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*How do we analyse social science text data?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we analyse social science text data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a wide array of text analysis techniques that we could apply in our research:\n",
    "* **Descriptive inference:** how to characterise text; vector space model, bag of words, (dis)similarity measures, diversity, complexity, style, bursts.\n",
    "* **Supervised techniques:** dictionaries, sentiment analysis, categorising.\n",
    "* **Unsupervised techniques:** cluster analysis, Principal Components Analysis (PCA), topic modelling, embeddings. (Spirling, 2022)\n",
    "\n",
    "To say nothing of using Generative AI or Large Language Models (LLMs) to conduct these analyses on our behalf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we focus on a common unsupervised text analysis technique:\n",
    "* Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First we need to ensure Python has the functionality it needs for text analysis. As you will see, it needs quite a bit of extra functionality, so this may take some time to install / import depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages - only run once per machine\n",
    "!pip install textblob\n",
    "!pip install seaborn\n",
    "!pip install pyldavis\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for general data and file management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for processing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)  \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words     # list of valid words\n",
    "english_words = set(words.words())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import wordnet                    # Functions we need for lemmatising\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer         # Functions we need for stemming\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for analysing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# for data visualisation\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # for data visualisation\n",
    "\n",
    "# for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for topic modelling\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# for topic modelling evaluation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second important preliminary step is to import the text data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"https://raw.githubusercontent.com/SGSSSonline/text-analysis-summer-school-2025/refs/heads/main/data/acnc-overseas-activities-2022.csv\" # define file to be imported\n",
    "\n",
    "data = pd.read_csv(infile, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_desc\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Create Document Term Matrix\n",
    "\n",
    "You have likely created and saved this in a previous lesson but let's start afresh just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text and convert to lowercase\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    #print(lower_words)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    a_words = [word for word in lower_words if word.isalpha()]\n",
    "    #print(\"Alpha words: \",a_words)\n",
    "\n",
    "    # Lemmatise words\n",
    "    lemmed_words = [lemmatizer.lemmatize(word) for word in a_words]\n",
    "    #print(\"Lemmed words: \",lemmed_words)\n",
    "    \n",
    "    # Remove non-English words\n",
    "    e_words = [word for word in lemmed_words if word in english_words]\n",
    "    #print(\"English words: \", e_words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stop_words = [\"registered\", \"registration\", \"company\", \"number\", \"australia\", \n",
    "                      \"australian\", \"report\", \"charity\", \"charities\", \"charitable\", \"year\", \n",
    "                      \"end\", \"statement\", \"statements\", \"trustee\", \"trustees\", \"trust\", \"overseas\",\n",
    "                     \"international\", \"support\", \"fund\", \"provide\", \"provision\", \"activity\", \"activities\",\n",
    "                     \"providing\", \"provided\", \"program\", \"programme\", \"project\"]\n",
    "    stop_words.update(new_stop_words)\n",
    "    s_words = [word for word in e_words if word not in stop_words]\n",
    "    #print(\"Stop words: \", s_words)\n",
    "\n",
    "    # Stem words\n",
    "    #stemmed_words = [porter.stem(word) for word in p_words]\n",
    "\n",
    "    # Remove words with less than three characters\n",
    "    clean_words = [word for word in s_words if len(str(word)) > 2]\n",
    "\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is valid\n",
    "data[\"activity_desc\"] = data[\"activity_desc\"].astype(str)\n",
    "data = data.dropna(subset=[\"activity_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"activity_desc\"].apply(preprocess_text)\n",
    "data[[\"abn\", \"activity_desc\", \"clean_text\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to loop over every row in the dataset and extract the charity unique id and the cleaned activity description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(row[\"abn\"], row[\"clean_text\"]) for _, row in data.iterrows()]\n",
    "documents[0:5] # view first five elements in list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the cleaned text for converting to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [text for _, text in documents]\n",
    "text_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Document-Term Matrix using a Count or TF-IDF vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\r",
    "bow = vectorizer.fit_transform(text_data)\n",
    "terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()\r",
    "#bow = vectorizer.fit_transform(text_data)\n",
    "#terms = vectorizer.get_feature_names_out() # extract unique terms in corpus (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM into a Pandas DataFrame\n",
    "dtm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out(), index=[doc_id for doc_id, _ in documents])\n",
    "document_ids = dtm.index.tolist() # create list of document ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', None) # change display options\n",
    "pd.set_option('display.max_columns', None) # change display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms[0:500]) # view first 500 terms in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unsupervised text analysis technique (or unsupervised learning more generally) is one that seeks to uncover or determine what category or class a document belongs to. Cluster analysis is a type of unsupervised learning technique as it groups observations according to shared characteristics or features. We do not know *a priori* what group a document belongs to, we need to estimate or predict based on the features of the document. This and similar techniques (e.g., PCA, topic modelling) are termed **unsupervised** because the category or class is not already known and the text analysis technique is therefore not guided or supervised as to what the correct categories or classes are.\n",
    "\n",
    "Perhaps documents share a linguistic style, or talk about similar topics at similar rates. Unless we want to read a lot of these documents and manually code them into particular categories or classes, we need to use techniques that construct these groups from the ground up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models are an important intellectual development in the social sciences (Blei, 2012). Topic modelling is a technique for discovering the main themes in a corpus of documents. At its simplest these topics can be used to organise the documents in a corpus; that is, are there groups of documents talking about similar topics? However topic models can also be used to measure and explain the prevalence of interesting themes across documents e.g., do small charities talk about a certain topic more frequently than medium and large organisations?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get straight into this technique using the full charity activity DTM. We will use a package we haven't seen before (`gensim`) so don't worry if the code is unfamiliar: we just need a couple of simple steps to convert our DTM to a format that package can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[word for word, freq in zip(dtm.columns, row) for _ in range(freq)] for row in dtm.to_numpy()]\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below = 10, no_above= .95) # words must appear in at least 10 documents, and no more than 95% of documents\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dictionary size after filtering: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models can take quite long to run depending on the size of the corpus, how many topics we are looking for, how many passes through the corpus we take etc. Therefore it can help if we limit the scope of the topic modelling. We do this in the above code block by only including words that appear in at least 10 documents and in no more than 95% of documents.\n",
    "\n",
    "Notice that the filtering of very rare and very common terms has reduced the number of unique terms (or types_ from c.4,200 to c.800. This may not be optimal but is a good starting point for our first topic model. (If you want to avoid any filtering, just place a '#' at the beginning of `dictionary.filter_extremes(no_below = 10, no_above= .95)` above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's estimate a topic model with five topics, and let's have the model pass over each document 10 times, and the corpus as a whole 10 times. (Think of this as a human reading each document ten times and then going over the whole corpus ten times before determining what the five topics are.) In addition we set a seed, which is a way of ensuring the topic model generates the same results every time. Remember that topic models are **probabilistic** by design: they estimate the distribution of documents over topics, and topics over words. These distributions change slightly every time you estimate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 37\n",
    "topic_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, iterations=10, passes=10, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error messages means the code ran successfully but what about the results? First let's extract the topics and the top 20 words associated with each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "for topic_id in range(topic_model.num_topics):\n",
    "    top_words = topic_model.show_topic(topic_id, topn=20)\n",
    "    topics[f\"Topic {topic_id+1}\"] = [word for word, _ in top_words]\n",
    "\n",
    "# Convert topics to DataFrame for display\n",
    "topics_df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the proportion of a topic that is constituted by each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_model.print_topics(num_topics=5, num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Increase the number of words (`num_words=10`) in the above code to see what proportion other words contribute to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the number of unique terms associated with each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts = {f\"Topic {i+1}\": len(topic_model.show_topic(i, topn=len(dictionary))) \n",
    "                     for i in range(topic_model.num_topics)}\n",
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like every term is associated with every topic, which is not informative. However this is expected: all documents share the same topics, and all topics share the same words. Where topics differ is in the probabilties each word has with each topic. Put another way, what is the contribution of each term to a topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts = {\n",
    "    f\"Topic {i+1}\": sum(1 for _, prob in topic_model.show_topic(i, topn=len(dictionary)) if prob > 0.01) \n",
    "    for i in range(topic_model.num_topics)\n",
    "}\n",
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there are only a small number of unique terms per topic that make up a meaningful proportion of the topic (i.e., more than 1% probability of being associated with a topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular representations of the topics can be informative but visualisations are an excellent way of deriving deeper insight from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare visualization\n",
    "tm = gensimvis.prepare(topic_model, corpus, dictionary, mds='mmds', sort_topics=True)\n",
    "\n",
    "# Save as HTML\n",
    "pyLDAvis.save_html(tm, 'topic-model.html')\n",
    "\n",
    "# Display visualization\n",
    "pyLDAvis.display(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation can be overwhelming so here is some practical guidance for interpreting the findings:\n",
    "* Look at the left panel (scatterplot of topics): Are topics well-separated, or do they overlap?\n",
    "* Click on topics one by one: What are the key words in each topic?\n",
    "* Adjust the λ slider: Does it help clarify the topics? Which words are exclusive to a particular topic (choose a small lambda value)?\n",
    "* Compare the top words: Do they align with expected themes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial interpretation of topic 4 suggests it is about religion. Words like \"church\", \"ministry\", and \"faith\" are both common to and highly associated with this topic: that is, these words appear frequently in relation to this topic **and** when these words are used in the corpus, it is almost entirely in relation to this topic (adjust the λ slider to a lower value to see words that are not that common in the corpus overall but are to a given topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Select topic 5 and interpret the results. What theme do you think this topic represents? What words are common to this topic in terms of freqeuncy and proportion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovering the Document-Topic Matrix and the Topic-Term Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling produces two sets of results or matrices of interest:\n",
    "* The Document-Topic Matrix - the distribution of documents (rows) across topics (columns). The cells contain the probability of each topic appearing in a document.\n",
    "* The Topic-Term Matrix - the distribution of topics (rows) across terms (columns). The cells contain the probability of each word appearing in a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Topic Matrix\n",
    "doc_topic_matrix = []\n",
    "for doc_bow in corpus:\n",
    "    topic_distribution = topic_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    doc_topic_matrix.append([prob for _, prob in topic_distribution])\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "doc_topic_df = pd.DataFrame(doc_topic_matrix, columns=[f\"Topic {i+1}\" for i in range(topic_model.num_topics)])\n",
    "\n",
    "# Add original document ids\n",
    "doc_topic_df[\"doc_id\"] = document_ids # this is possible because topic modelling preserves row order from the DTM\n",
    "\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we validate whether the documents actually cohere with the topics? We have interpreted topic 4 as 'religion' though there are other words relating to education. Let's look at the documents that map most closely to topic 4 and read the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.sort_values(by='Topic 4', ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, document `87161085650` seems to be mainly about topic 4 ('religion'), as evidence by its high proportion (97% of the document's words are associated with this topic). Let's perform a close reading of this document to see if this really is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show full content in columns, including long text\n",
    "data.loc[data[\"abn\"] == 87161085650, [\"abn\", \"activity_desc\", \"clean_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, seems to be more about education than religion. Let's look at some others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"abn\"] == 76611738464, [\"abn\", \"activity_desc\", \"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth') # reset display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Term Matrix\n",
    "topic_term_matrix = topic_model.get_topics()\n",
    "topic_term_df = pd.DataFrame(topic_term_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\n",
    "topic_term_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know how many topics is optimal? Well this is really a subjective task, as it is the analyst's judgement that matters most here. There are \"objective\" approaches we could take. One of these is called the *coherence score*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model = CoherenceModel(model=topic_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherence score: \", coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence is used to compare different topic models, especially where the number of topics differs. Values closer to 1 represent a set of topics that are semantically consistent: there are clear differences in the groups of words forming different topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for text analysis tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to perform unsupervised text analyses**. There are a number of common and key analytical techniques that can yield substantive insight into key features of documents.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "These are but a selection of the analytical techniques at your displosal; however they are common and often key ones in text analysis projects. Topic modelling in particular is a comprehensive analytical technique that deserves deeper engagement and practice with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform topic modelling using the other file in the data folder (*acnc-overseas-activities-2021.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we introduce and apply the foundational preprocessing steps of text analysis for social science research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use Python to preprocess text data relating to charitable activities.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data preprocessing problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 40-60 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand the key steps and concepts for getting social science data ready for text analysis.\n",
    "    2. Be able to use Python for preprocessing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*How do we prepare social science data for text analysis?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we prepare social science data for text analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of common, initial steps before you can perform text analysis with social science data. Grimmer et al., (2022) suggest the following workflow (Grimmer et al., 2022):\n",
    "1. Choose unit of analysis\n",
    "2. Tokenise\n",
    "3. Reduce complexity:\n",
    "   * Convert to lowercase\n",
    "   * Remove punctuation\n",
    "   * Remove stop words\n",
    "   * Create equivalence classes (lemmatisation / stemming)\n",
    "   * Filter by frequency\n",
    "4. Construct document-feature matrix (W = N*J) (Wij = count of type j in document i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First we need to ensure Python has the functionality it needs for text analysis. As you will see, it needs quite a bit of extra functionality, so this may take some time to install / import depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages - only run once per machine\n",
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for general data and file management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for processing text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                       # get nltk \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "from nltk import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "from autocorrect import Speller   # things we need for spell checking\n",
    "check = Speller(lang='en')\n",
    "\n",
    "nltk.download('stopwords') # additional words or dictionaries we can use to check our documents against\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words     # list of valid English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "from nltk.corpus import stopwords # list of common words that are not substantively informative e.g., \"the\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import wordnet                    # functions we need for lemmatising\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer         # function we need for stemming\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # function we need for converting text to numeric\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # function we need for converting text to numeric\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what modules we need for text analysis? Thankfully it is an established method, therefore others have figured this out for us:\n",
    "* https://github.com/UKDataServiceOpen/text-mining/blob/407d16015ba270b4e39462c20de9b370c4e78563/code/1-Processing.ipynb\n",
    "* https://github.com/UKDataServiceOpen/text-mining/blob/407d16015ba270b4e39462c20de9b370c4e78563/code/2-Extraction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Modules are additional techniques or functions that are not present when you launch Python. Some do not even come with Python when you download it and must be installed on your machine separately - think of using `ssc install <package>` in Stata, or `install.packages(<package>)` in R. For now just understand that many useful modules need to be imported every time you start a new Python session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second important preliminary step is to import the text data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"https://raw.githubusercontent.com/SGSSSonline/text-analysis-summer-school-2025/refs/heads/main/data/acnc-overseas-activities-2022.csv\" # define file to be imported\n",
    "\n",
    "data = pd.read_csv(infile, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_desc\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Choose unit of analysis\n",
    "\n",
    "A fundamental task in social science research more generally, it is important for text analysis also. In many cases the unit of analysis is the **document**; that is, we are interested in measuring relevant, salient features of a document (e.g., author, style, sentiment, topics) and comparing these across other documents. However we can also select other, often smaller units of analysis such as paragraphs or sentences - then we can compare *within* and *between* documents e.g., how do political speeches develop from beginning to end, and across different speakers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis the unit of analysis is the **document**: each row in the raw data represents a single charity's description of its overseas activities. This description is what serves as the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before unleashing this workflow on a corpus, let's apply it to a single document so we can get a sense of what happens at each step. Below we select the text in the `activity_desc` column for the 501st row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = data.iloc[500, data.columns.get_loc(\"activity_desc\")]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Read through the above activity description and note any issues with the text e.g., misspellings, odd words, improper punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next major step is to split the text into subunits of analysis. The most common subunit of interest is a type (or word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = word_tokenize(sample_text)\n",
    "print(sample_words)  # comma-separated list of tokens in the text\n",
    "print(len(sample_words)) # number of tokens in text\n",
    "print(len(set(sample_words))) # number of types in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that tokenising is an important but not infallible step in preprocessing text data. Tokenisers generally work by splitting text into separate components. How do these approaches know when one component (e.g., word) begins and another ends? They use whitespace as a delimiter / separator. This works very well but not perfectly, as you can see punctuation like commas, periods and brackets are identified as tokens in the text. We are generally not interested in punctuation for analysis, so we need a later step to remove these instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tokenise the text into sentences if these were our linguistic subunits of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent = sent_tokenize(sample_text)\n",
    "print(sample_sent) # comma-separated list of sentences in the text\n",
    "print(len(sample_sent)) # number of sentences in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless capitalisation is of analytical interest, we generally convert all tokens to lowercase. In essence we want to avoid situations where we treat the same words as if they different e.g., are \"The\" and \"the\" different words? \"Charity\" and \"CHARITY\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lower = [word.lower() for word in sample_words]\n",
    "print(sample_lower)\n",
    "print(len(sample_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spell check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is often not necessary and can be computationally intensive. However here is how you can do it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "check = Speller(lang='en')\n",
    "sample_correct_spell = []\n",
    "\n",
    "for word in sample_lower:\n",
    "    sample_correct_spell.append(check(word))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_correct_spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_punct = [w.translate(table_punctuation) for w in sample_lower]  \n",
    "print(sample_no_punct)\n",
    "print(len(sample_no_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there are still some issues:\n",
    "* There are spaces operationalised as tokens e.g., where there used to be punctuation\n",
    "* There are tokens consisting of a single character e.g., a number or letter that was separated from its apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_space = list(filter(None, sample_no_punct)) # strip out empty spaces\n",
    "print(sample_no_space)\n",
    "print(len(sample_no_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text looks much cleaner and we have reduced the number of tokens as well (good for computational efficiency and substantive clarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_stop_words = []\n",
    "\n",
    "for word in sample_no_space:\n",
    "    if word not in stop_words:\n",
    "        sample_no_stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_no_stop_words)\n",
    "print(len(sample_no_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create equivalence classes: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stemmed = [porter.stem(word) for word in sample_no_stop_words]\n",
    "print(sample_stemmed)\n",
    "print(len(sample_stemmed)) # number of tokens\n",
    "print(len(set(sample_stemmed))) # number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what has happened words like \"comply\", \"external\" and \"charity\". They are now expressed in their common root form and thus are no longer words that we would find in the English dictionary. These are examples of terms rather than types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What is the value of transforming words to their root form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create equivalence classes: Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an alternative to stemming that maps words to a common word based on semantic meaning e.g., \"car\" and \"cars\" map to \"car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lemmed = [lemmatizer.lemmatize(word) for word in sample_no_stop_words]\n",
    "print(sample_lemmed)\n",
    "print(len(sample_lemmed)) # number of tokens\n",
    "print(len(set(sample_lemmed))) # number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What is the difference between stemming and lemmatisation in the example above, both in terms of the number of terms / tokens and the readability of the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we may want to remove very common or very rare words from the corpus: this aids both substantive interpretations (e.g., perhaps all charities mention their beneficiaries in their activity descriptions) or certain words only appear once across the entire corpus (e.g., misspellings or acroynms).\n",
    "\n",
    "We can view the frequency table of the terms in our corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = Counter(sample_lemmed)\n",
    "print(freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are only working with one document at the moment we won't remove any words just yet. However there are better approaches for handling common / rare terms in a corpus that we shall see shortly (e.g., weighting). For completeness sake, here is how you could remove words based on their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = max(freq_table.values()) # find most frequency word(s)\n",
    "min_count = min(freq_table.values()) # find most frequency word(s)\n",
    "\n",
    "sample_filtered = [word for word in sample_lemmed if freq_table[word] not in (max_count, min_count)] # create new list of terms\n",
    "\n",
    "print(\"Original tokens:\", sample_lemmed)\n",
    "print(\"Filtered tokens:\", sample_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with the preprocessing steps above, both in terms of effect and order, we can convert the text to a numeric format suitable for analysis. This format is known as a Document-Term Matrix (DTM) or Document-Feature Matrix (DFM) - the latter is a more general format than the former. Both simply represent a document or corpus in a tabular format, where every row represents a document and every column represents a term or feature relating to the document. If you are a quantitative researcher then this format will be familiar to you e.g., the rows are units of analysis and the columns are variables representative numeric characteristics of the units of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to the create the DTM we need to convert the list of terms into a single string of terms as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \" \".join(sample_lemmed)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the single string of terms and represent them in the \"bag of words\" format - there are a couple of ways of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = FreqDist(sample_lemmed)\n",
    "#print(dict(bow))\n",
    "dtm = pd.DataFrame([bow])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([sample_text])\n",
    "dtm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the vocabulary (unique words) and the bag of words matrix\n",
    "#vocabulary = vectorizer.get_feature_names_out()\n",
    "#bow_matrix = bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What are the differences between the two approaches (look at the data frame content and shape results)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be difficult to see in the notebook but most DTMs based on real social science text data are *sparse*: that it, there are lots of terms with zero counts for many documents in the corpus. This is a function of the nature of language (authors have lots of words to choose from when creating a given document) and if any reweighting of terms has been applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pilot end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew, that is a lot of preprocessing and quite a bit of code to get your head around. The good news is these tasks are common to almost all text analysis projects, so once you get your head around them you will be set for future work. And we don't have to run through all of these steps in such a manual way either: we can rationalise our code and make use of functions from packages like `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could still perform some additional work to improve the substantive relevance of the text:\n",
    "* Remove numbers\n",
    "* Remove single-character tokens\n",
    "* Remove subject-specific stop words (e.g., \"charity\", \"charities\", \"australia\", \"year\", \"trust\", \"fund\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the full DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the DTM we will use for analysis. Instead of sampling one document we will preprocess all of them and make some simple adjustments to improve the text cleaning (e.g., removing numbers and common stop words). To speed up this process, let's create a function (block of code) that handles all of these steps in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text and convert to lowercase\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    #print(lower_words)\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    a_words = [word for word in lower_words if word.isalpha()]\n",
    "    #print(\"Alpha words: \",a_words)\n",
    "\n",
    "    # Lemmatise words\n",
    "    lemmed_words = [lemmatizer.lemmatize(word) for word in a_words]\n",
    "    #print(\"Lemmed words: \",lemmed_words)\n",
    "    \n",
    "    # Remove non-English words\n",
    "    e_words = [word for word in lemmed_words if word in english_words]\n",
    "    #print(\"English words: \", e_words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_stop_words = [\"registered\", \"registration\", \"company\", \"number\", \"australia\", \n",
    "                      \"australian\", \"report\", \"charity\", \"charities\", \"charitable\", \"year\", \n",
    "                      \"end\", \"statement\", \"statements\", \"trustee\", \"trustees\", \"trust\", \"overseas\",\n",
    "                     \"international\", \"support\", \"fund\", \"provide\", \"provision\", \"activity\", \"activities\",\n",
    "                     \"providing\", \"provided\", \"program\", \"programme\", \"project\"]\n",
    "    stop_words.update(new_stop_words)\n",
    "    s_words = [word for word in e_words if word not in stop_words]\n",
    "    #print(\"Stop words: \", s_words)\n",
    "\n",
    "    # Stem words\n",
    "    #stemmed_words = [porter.stem(word) for word in p_words]\n",
    "\n",
    "    # Remove words with less than three characters\n",
    "    clean_words = [word for word in s_words if len(str(word)) > 2]\n",
    "\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What are the consequences of removing non-English words from the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is valid\n",
    "data[\"activity_desc\"] = data[\"activity_desc\"].astype(str)\n",
    "data = data.dropna(subset=[\"activity_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"activity_desc\"].apply(preprocess_text)\n",
    "data[[\"abn\", \"activity_desc\", \"clean_text\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to loop over every row in the dataset and extract the charity unique id and the cleaned activity description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(row[\"abn\"], row[\"clean_text\"]) for _, row in data.iterrows()]\n",
    "documents[0:5] # view first five elements in list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the cleaned text for converting to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [text for _, text in documents]\n",
    "text_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Document-Term Matrix using a Count or TF-IDF vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\r",
    "#dtm = vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\r",
    "bow = vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM into a Pandas DataFrame\n",
    "dtm = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out(), index=[doc_id for doc_id, _ in documents])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', None) # change display options\n",
    "pd.set_option('display.max_columns', None) # change display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DTM as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary downloads folder\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./tmp\")\n",
    "except:\n",
    "    print(\"Unable to create folder: already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.to_csv(\"./tmp/acnc-2022-activities-dtm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for text analysis tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to preprocess text using a standard workflow**. There are a number of preprocessing steps common to almost all text analysis projects but you still retain some control over which steps and in which order you apply them.\n",
    "* **How to convert text to a number format**. The DTM / DFM is the workhorse of text analysis as it offers an efficient format for performing calculations on salient terms or features of text.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "There are a number of important steps in getting text data ready for analysis. However you need to think carefully about how sensitive your findings are to variation in the preprocessing steps or order. We will see why we go to the effort of creating a DTM / DFM in the next practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DTM / DFM using the other file in the data folder (*acnc-overseas-activities-2021.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is provided at the end of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a DTM for 2021 data on overseas charitable activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
